<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
    <meta name="format-detection" content="telephone=no" />

        <link rel="shortcut icon" href="header/mars_logo2c.png"/>
		<link href="css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
		<link href="css/font.css" rel="stylesheet" />
		<link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
		<link href="css/animsition.min.css" rel="stylesheet"/>
		<link href="css/simpletextrotator.css" rel="stylesheet"/>
		<link href="css/magnific-popup.css" rel="stylesheet"/>
		<link href="css/style.css" rel="stylesheet" type="text/css"/>
      <title>MARS - DATASET</title>
  </head>
<body class="animsition">
  <header>
    <div class="logo">
     <a href="index.html"></a>
    </div>
    <div class="nav-menu-icon">
    <a href="#"><i></i></a>
    </div>
    <nav class="menu">
    <ul>
      <li class="active"><a href="index.html">home<span class="fa"></span></a></li>
      <li><a href="https://github.com/neuroethology">GitHub<span class="fa fa-angle-down"></span></a>
      <ul class="dropmenu">
           <li><a href="https://github.com/neuroethology/MARS" class="animsition-link">MARS</a></li>
           <li><a href="https://github.com/neuroethology/MARS_Developer" class="animsition-link">MARS_Developer</a></li>
           <li><a href="https://github.com/neuroethology/bentoMAT" class="animsition-link">Bento</a></li>
        </ul>
      </li>
      <li><a href="dataset.html">datasets<span class="fa fa-angle-down"></span></a>
        <ul class="dropmenu">
          <li><a href="datasets.html" class="animsition-link">About the datasets</a></li>
           <li><a href="https://data.caltech.edu/records/2011" class="animsition-link">Download pose annotations</a></li>
           <li><a href="https://data.caltech.edu/records/2012" class="animsition-link">Download behavior annotations</a></li>
           <li><a href="https://data.caltech.edu/records/2121" class="animsition-link">Download inter-annotator variability</a></li>
        </ul>
      </li>
      <li><a href="team.html">team<span class="fa"></span></a></li>
    </ul>
    </nav>
  </header>

	<div class="container">
		<div class="padd-80">
			<div class="row">
				<div class="col-md-12">
					<figure class="figure">
						<a><img src="header/dataset.png" alt="" style="width:100%;height:auto;" class="center"></a>
					</figure>
				</div>
			</div>
		</div>
	</div>


    <div class="main-wrapp start-line time-line">
       <div class="container">
       	    <div class="padd-80">
        	  <div class="row">
        		<div class="col-md-12">
       		      <div class="about-item">
        		      <p style="text-align:justify;font-size:16px">
					  MARS classifiers were trained on a dataset of <em><b>108x2 videos</b></em> (recorded at 30Hz with <em><b>synchronized top and side view</em></b>) of pairs of mice engaging in <em><b>social behavior</em></b>. The dataset contains over <em><b>1.5M frames</em></b> (~<em><b>14 hours</em></b>) of behavior video from each view. Each video lasts betwen 2 and 10min and is accompanied by frame-by-frame manual annotation of social behaviors by an expert human annotator.
					  </br></br>
					  Video and annotations are accompanied by the tracked pose (<em><b>bounding boxes</em></b> around each mouse
					  and <em><b>keypoint locations</em></b>), and 270 <em><b>time-varying trajectory features</em></b> computed from the poses of the mice.
					  Further we share the selected <em><b>15Kx2 frames and AMT annotations (~5M clicks)</em></b> used to train the tracker.
					  </p>

					  </br></br></br>

					  <h6 class="hrline"><span>Pose Annotations</span></h6>
            <br><br>
            <ul>
              <li><span  class="fa fa-square-o"></span> <b><a href=https://data.caltech.edu/records/2011>Download the MARS pose dataset here!</a></b></li>
            </ul>
            <br></br><br>
					  <p style="text-align:justify;font-size:16px">
            Included in this dataset are 15,000 pairs of top- and front-view frames from videos of pairs of interacting mice. Each frame has been manually annotated by five individuals for a total of nine (top-view) or thirteen (front-view) keypoints: the nose, ears, base of neck, hips, base of tail, middle of tail, and end of tail, and additionally the four paws (front-view only.)
            <br><br>
            Frames are sampled from 64 videos from several years of experimental projects. 5,000 of the extracted frames include resident mice with a head-attached fiberoptic cable or head-mounted microendoscope with cable, which introduces an additional source of occlusion.
            <br><br>
            Note that the middle and end of tail keypoints were quite noisy, therefore these parts were omitted during training of the original MARS models.
					  </p>
					  </br>
					  <h6 class="hrline"><span>Behavior annotations</span></h6>
          </br><br>
          <ul>
            <li><span  class="fa fa-square-o"></span> <b><a href=https://data.caltech.edu/records/2012>Download the MARS behavior dataset here!</a></b></li>
          </ul>
          <br></br><br>
					  <p style="text-align:justify;font-size:16px">
					  Included in this dataset are approximately 14 hours of top-view video of pairs of interacting mice, accompanied by MARS pose estimator output (keypoint locations, bounding boxes, and derived features) and manual frame-by-frame annotations. This data was used to train the attack, mount, and close investigation classifiers in end-user version of MARS.
            <br><br>
            The videos are split into train, validation, test-1, and test-2 sets, which reflect the train/test splits used in the MARS paper.
            <br><br>
					  Each video folder contains top and front videos in .seq and .avi format, behavior annotations and an output_v# folder.
					  Some videos contain two versions of behavior annotations: one where only aggression, investigation and mounting are annotated, where
					  the other provides in total 9 behaviors. </br>
					  The output_v# folder is the outcome from MARS and # stands for the MARS version. It containes bounding boxes and pose extracted from both view, and extracted features only from top view,
					  top view with front view pixel change features (pcf) and front view only. </br></br>

					  Pose files (.json) contains:</br>
					  &mdash; bounding boxes [2 x num_frames x 4] </br>
					  &mdash; pose [2 x num_frames x num_keypoints_view] </br>
					  </p>



          </br>
          <h6 class="hrline"><span>Inter-Annotator Variability</span></h6>
        </br><br>
        <ul>
          <li><span  class="fa fa-square-o"></span> <b><a href=https://data.caltech.edu/records/2121>Download the multi-worker behavior annotation dataset here!</a></b></li>
        </ul>
        <br></br><br>
          <p style="text-align:justify;font-size:16px">
          Included in this dataset are 10x2 top- and front-view videos of pairs of interacting mice, accompanied by MARS pose estimator output (keypoint locations, bounding boxes, and derived features) and manual frame-by-frame behavior annotations by each of eight trained individuals. All annotations are for attack, mount, and close investigation behaviors.
          <br><br>
          Two of the ten videos were annotated a second time by each individual for quantification of intra-annotator variability.
          </p>

					  </br></br></br>






                  </div>
        		</div>
        	  </div>
           </div>





       </div>
    </div>

	<footer>
		<div class="copyright">

		</div>
	</footer>

<script src="js/jquery-2.1.4.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/isotope.pkgd.min.js"></script>
<script src="js/jquery.countTo.js"></script>
<script src="js/jquery.animsition.min.js"></script>
<script src="js/jquery.magnific-popup.min.js"></script>
<script src="js/idangerous.swiper.min.js"></script>
<script src="js/all.js"></script>
</body>
</html>
